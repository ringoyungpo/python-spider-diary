---
title: "网页爬取要点"
author: Ringo Yungpo Kao
date: December 23, 2017
output:
  word_document:
    path: /Exports/网页爬取要点.docx
---
# 网页爬取要点
## 静态网页抓取
### 获取响应内容
```Python
#!/usr/bin/python
# coding: utf-8

import requests

link = "http://www.jmu.edu.cn/"
headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'} 

r = requests.get(link, headers= headers)
print("文本编码:",r.encoding)
print("响应状态码:",r.status_code)
print("字节方式响应体:",r.content)
print("字符串方式的响应体:",r.text)
print("JSON解码器:",r.json)
```
```PowerShell
文本编码: ISO-8859-1
响应状态码: 200
字节方式响应体: b'<html>\r\n<head>\r\n<script language="javascript">setTimeout("location.replace(location.href.split(\\"#\\")[0])",1000);</script>\r\n</head>\r\n<iframe src="http://210.34.132.69:89/flashredir.html" frameborder=0></iframe>\r\n</html>\r\n\r\n'
字符串方式的响应体: <html>
<head>
<script language="javascript">setTimeout("location.replace(location.href.split(\"#\")[0])",1000);</script>
</head>
<iframe src="http://210.34.132.69:89/flashredir.html" frameborder=0></iframe>
</html>

JSON解码器: <bound method Response.json of <Response [200]>>
```

- 可以看到学校的文本编码格式和普通的UTF-8不大一样，估计会在字符集上面出问题。
爬取
- 响应码是200，代表成功
- 字节响应提内容上比较难以看懂。
- 字符串响应体的内容上，正常的网站会返回一个页面，而学校是返回内网的一个flashredir.html页面，这也是很大的问题。
- JSON解码器的来源可以看到

## 定时Requests
有些网页需要对Requests的参数进行设置才能访问，比如校园网如果不把headers设置为浏览器方式的话就不能访问。
### 传递URL参数
```Python
import requests
key_dict = {'key1': 'value1', 'key2': 'value2'}
r = requests.get('http://httpbin.org/get', params=key_dict)
print ("URL已经正确编码:", r.url)
print ("字符串方式的响应体: \n", r.text)
```
```PowerShell
URL已经正确编码: http://httpbin.org/get?key1=value1&key2=value2
字符串方式的响应体: 
 {
  "args": {
    "key1": "value1", 
    "key2": "value2"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Connection": "close", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.18.4"
  }, 
  "origin": "110.34.180.19", 
  "url": "http://httpbin.org/get?key1=value1&key2=value2"
}
```
- 可以看到在发送GET请求的时候通过params = key_dict更改内部参数可以获得对应url地址。
- 其中httpbin.org可以把请求内容响应出来。
## 发送POTST请求
```Python
import requests
key_dict = {'key1': 'value1', 'key2': 'value2'}
r = requests.post('http://httpbin.org/post', data=key_dict)
print (r.text)
```
```PowerShell
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "key1": "value1", 
    "key2": "value2"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Connection": "close", 
    "Content-Length": "23", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.18.4"
  }, 
  "json": null, 
  "origin": "110.34.180.19", 
  "url": "http://httpbin.org/post"
}
```
可以看到post请求方式下通过data传递请求体内容
```Python
import requests
link = "http://www.santostang.com/" 
r = requests.get(link, timeout= 0.01)
```
为了避免无限等待，通过timeout设置超时时间，单位为秒

## TOP250电影数据抓取实战
### 网站分析
第一页url为`https://movie.douban.com/top250`
第二页url为`https://movie.douban.com/top250?start=25&filter=`
第三页url为`https://movie.douban.com/top250?start=50&filter=`
有理由推断爬取链接为`link = 'https://movie.douban.com/top250?start=' + str(i * 25)`
爬取内容为第i+1页数据
i在0~9之间
通过分析页面代码可以发现电影标题在`class_='hd'`的div内
可以用BeautifulSoup来解析单页标题数据所在div为为`soup.find_all('div', class_='hd')`
### 编码实战
```Python
import requests
from bs4 import BeautifulSoup

def get_movies():
    headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.82 Safari/537.36',
    'Host': 'movie.douban.com'
    }
    movie_list = []
    for i in range(0,10):
        link = 'https://movie.douban.com/top250?start=' + str(i * 25)
        r = requests.get(link, headers=headers, timeout= 10)
        print (str(i+1),"页响应状态码:", r.status_code)
        
        soup = BeautifulSoup(r.text, "lxml")
        div_list = soup.find_all('div', class_='hd')
        for each in div_list:
            movie = each.a.span.text.strip()
            movie_list.append(movie)
    return movie_list
        
movies = get_movies()
print (movie_list)
```